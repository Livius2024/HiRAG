{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# HiRAG Fiscalitate Chatbot (versiune profesional\u0103)\n",
    "\n",
    "Acest notebook Colab preg\u0103te\u0219te un flux complet pentru a \u00eenc\u0103rca textul fiscal `codfiscal.txt` \u0219i a construi un asistent RAG bazat pe **HiRAG** (modul ierarhic complet). Vei putea indexa corpusul fiscal \u0219i rula \u00eentreb\u0103ri \u00een limba rom\u00e2n\u0103 folosind acela\u0219i pipeline profesional din scripturile HiRAG.\n",
    "\n",
    "Implicit, notebook-ul folose\u0219te modelul `gpt-5-mini` \u0219i embedding `text-embedding-3-large` (3072 dim), iar grafurile sunt stocate \u00een Neo4j Aura cu conexiunile de mai jos. Po\u021bi schimba rapid furnizorul sau baza de date direct din celule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalare \u0219i configurare proiect",
    "",
    "Rul\u00e2nd celula de mai jos vei clona automat repository-ul (dac\u0103 nu exist\u0103 deja), vei intra \u00een directorul lui \u0219i vei instala dependin\u021bele prin `pip install -e .` pentru a folosi implementarea oficial\u0103 HiRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import sys",
    "import subprocess",
    "from pathlib import Path",
    "",
    "REPO_URL = \"https://github.com/Livius2024/HiRAG.git\"",
    "PROJECT_ROOT = Path.cwd()",
    "",
    "if PROJECT_ROOT.name != \"HiRAG\":",
    "    project_dir = PROJECT_ROOT / \"HiRAG\"",
    "    if not project_dir.exists():",
    "        subprocess.run([\"git\", \"clone\", REPO_URL], check=True)",
    "    os.chdir(project_dir)",
    "",
    "print(f\"Using project root: {Path.cwd()}\")",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"pip\"], check=True)",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setarea cheilor \u0219i a modelului LLM\n",
    "Completeaz\u0103 cheile API pentru furnizorul ales (OpenAI/DeepSeek/GLM). Valorile pot fi furnizate prin environment variables \u00een Colab (`%env OPENAI_API_KEY=...`) sau direct \u00een variabilele de mai jos. Modelul de completare \u0219i cel de embedding sunt preconfigurate pentru `gpt-5-mini` \u0219i `text-embedding-3-large` (3072 dimensiuni).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Alege furnizorul LLM: 'openai', 'deepseek' sau 'glm'\n",
    "PROVIDER = os.getenv(\"HIRAG_PROVIDER\", \"openai\").lower()\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "\n",
    "# DeepSeek\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n",
    "DEEPSEEK_BASE_URL = os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\n",
    "DEEPSEEK_MODEL = os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\n",
    "DEEPSEEK_EMBEDDING_MODEL = os.getenv(\"DEEPSEEK_EMBEDDING_MODEL\", \"deepseek-embedding\")\n",
    "\n",
    "# GLM (Zhipu)\n",
    "GLM_API_KEY = os.getenv(\"GLM_API_KEY\", \"\")\n",
    "GLM_BASE_URL = os.getenv(\"GLM_BASE_URL\", \"https://open.bigmodel.cn/api/paas/v4\")\n",
    "GLM_MODEL = os.getenv(\"GLM_MODEL\", \"glm-4-plus\")\n",
    "GLM_EMBEDDING_MODEL = os.getenv(\"GLM_EMBEDDING_MODEL\", \"embedding-3\")\n",
    "\n",
    "def ensure_key_set(provider: str):\n",
    "    if provider == \"openai\" and not OPENAI_API_KEY:\n",
    "        raise ValueError(\"Seteaz\u0103 OPENAI_API_KEY \u00eenainte de a continua.\")\n",
    "    if provider == \"deepseek\" and not DEEPSEEK_API_KEY:\n",
    "        raise ValueError(\"Seteaz\u0103 DEEPSEEK_API_KEY \u00eenainte de a continua.\")\n",
    "    if provider == \"glm\" and not GLM_API_KEY:\n",
    "        raise ValueError(\"Seteaz\u0103 GLM_API_KEY \u00eenainte de a continua.\")\n",
    "\n",
    "ensure_key_set(PROVIDER)\n",
    "print(f\"Furnizor selectat: {PROVIDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Configurarea Neo4j Aura\n",
    "Completeaz\u0103 datele de conectare pentru instan\u021ba Neo4j Aura. Valorile implicite corespund instan\u021bei `Codfiscal` furnizate, dar po\u021bi \u00eenlocui rapid cu propriile creden\u021biale.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution_count": null
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"neo4j+s://052d304f.databases.neo4j.io\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"Q0KWeSnDlidBtVCD634E532eBbBpL9Afaq-7grxCN1Y\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "AURA_INSTANCEID = os.getenv(\"AURA_INSTANCEID\", \"052d304f\")\n",
    "AURA_INSTANCENAME = os.getenv(\"AURA_INSTANCENAME\", \"Codfiscal\")\n",
    "\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"Seteaz\u0103 NEO4J_PASSWORD \u00eenainte de a continua.\")\n",
    "\n",
    "print(\n",
    "    f\"Neo4j Aura configurat pentru {NEO4J_URI} (db={NEO4J_DATABASE}, instan\u021b\u0103={AURA_INSTANCENAME}).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Desc\u0103rcarea corpusului fiscal\n",
    "`codfiscal.txt` este luat direct din repository \u0219i salvat local. Dup\u0103 prima rulare fi\u0219ierul este reutilizat, astfel \u00eenc\u00e2t pa\u0219ii de indexare s\u0103 fie rapizi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_URL = \"https://raw.githubusercontent.com/Livius2024/HiRAG/main/codfiscal.txt\"\n",
    "DATA_PATH = Path(\"data/codfiscal.txt\")\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    resp = requests.get(DATA_URL, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    DATA_PATH.write_text(resp.text, encoding=\"utf-8\")\n",
    "    print(f\"Desc\u0103rcat {DATA_URL} -> {DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"Folose\u0219te fi\u0219ierul existent: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Definirea func\u021biilor LLM/embedding (stil HiRAG profesional)\n",
    "Func\u021biile de mai jos reproduc stilul din scripturile oficiale (`hi_Search_*.py`), cu caching op\u021bional \u0219i batch-uri asincrone. Po\u021bi adapta furnizorul f\u0103r\u0103 a modifica logica restului notebook-ului.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from openai import AsyncOpenAI\n",
    "from hirag import HiRAG, QueryParam\n",
    "from hirag.base import BaseKVStorage\n",
    "from hirag._utils import EmbeddingFunc, compute_args_hash\n",
    "from hirag._storage import NetworkXStorage, Neo4jStorage\n",
    "\n",
    "@dataclass\n",
    "class _EmbeddingWrapper(EmbeddingFunc):\n",
    "    embedding_dim: int\n",
    "    max_token_size: int\n",
    "    func: callable\n",
    "\n",
    "    async def __call__(self, *args, **kwargs) -> np.ndarray:\n",
    "        return await self.func(*args, **kwargs)\n",
    "\n",
    "def embedding_wrapper(**kwargs):\n",
    "    def deco(func):\n",
    "        return _EmbeddingWrapper(func=func, **kwargs)\n",
    "    return deco\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    LLM_API_KEY = OPENAI_API_KEY\n",
    "    LLM_BASE_URL = OPENAI_BASE_URL\n",
    "    LLM_MODEL = OPENAI_MODEL\n",
    "    EMBEDDING_MODEL = OPENAI_EMBEDDING_MODEL\n",
    "    EMBEDDING_DIM = 3072 if \"text-embedding-3-large\" in OPENAI_EMBEDDING_MODEL else 1536\n",
    "elif PROVIDER == \"deepseek\":\n",
    "    LLM_API_KEY = DEEPSEEK_API_KEY\n",
    "    LLM_BASE_URL = DEEPSEEK_BASE_URL\n",
    "    LLM_MODEL = DEEPSEEK_MODEL\n",
    "    EMBEDDING_MODEL = DEEPSEEK_EMBEDDING_MODEL\n",
    "    EMBEDDING_DIM = 1536\n",
    "else:\n",
    "    LLM_API_KEY = GLM_API_KEY\n",
    "    LLM_BASE_URL = GLM_BASE_URL\n",
    "    LLM_MODEL = GLM_MODEL\n",
    "    EMBEDDING_MODEL = GLM_EMBEDDING_MODEL\n",
    "    EMBEDDING_DIM = 2048\n",
    "\n",
    "client_async = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)\n",
    "\n",
    "@embedding_wrapper(embedding_dim=EMBEDDING_DIM, max_token_size=8192)\n",
    "async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    result = await client_async.embeddings.create(model=EMBEDDING_MODEL, input=texts, encoding_format=\"float\")\n",
    "    return np.array([item.embedding for item in result.data])\n",
    "\n",
    "async def chat_completion(prompt: str, system_prompt: str | None = None, history_messages=None, **kwargs) -> str:\n",
    "    history_messages = history_messages or []\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.extend(history_messages)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    hashing_kv: BaseKVStorage | None = kwargs.pop(\"hashing_kv\", None)\n",
    "    args_hash = None\n",
    "    if hashing_kv is not None:\n",
    "        args_hash = compute_args_hash(LLM_MODEL, messages)\n",
    "        cached = await hashing_kv.get_by_id(args_hash)\n",
    "        if cached is not None:\n",
    "            return cached[\"return\"]\n",
    "\n",
    "    response = await client_async.chat.completions.create(model=LLM_MODEL, messages=messages, **kwargs)\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    if hashing_kv is not None:\n",
    "        await hashing_kv.upsert({args_hash: {\"return\": content, \"model\": LLM_MODEL}})\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Ini\u021bializarea grafului ierarhic HiRAG\n",
    "Instan\u021ba de mai jos activeaz\u0103 modul ierarhic (profesional), caching, embedding asincron \u0219i stocare grafic\u0103 \u00een Neo4j Aura. Po\u021bi schimba directorul de lucru pentru a p\u0103stra indec\u0219ii genera\u021bi \u00eentre rul\u0103ri.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "WORK_DIR = Path(\"hirag_codfiscal_cache\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "hirag = HiRAG(\n",
    "    working_dir=str(WORK_DIR),\n",
    "    enable_llm_cache=True,\n",
    "    enable_hierachical_mode=True,\n",
    "    enable_naive_rag=True,\n",
    "    embedding_func=embedding_func,\n",
    "    embedding_batch_num=6,\n",
    "    embedding_func_max_async=8,\n",
    "    best_model_func=chat_completion,\n",
    "    cheap_model_func=chat_completion,\n",
    "    graph_storage_cls=Neo4jStorage,\n",
    "    addon_params={\n",
    "        \"neo4j_url\": NEO4J_URI,\n",
    "        \"neo4j_auth\": [NEO4J_USERNAME, NEO4J_PASSWORD],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"HiRAG ini\u021bializat \u00een modul ierarhic complet (Neo4j).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Indexarea corpusului fiscal\n",
    "Aceast\u0103 celul\u0103 \u00eencarc\u0103 textul fiscal \u0219i \u00eel insereaz\u0103 \u00een graful ierarhic. Dac\u0103 rulezi de mai multe ori, indec\u0219ii existen\u021bi sunt reutiliza\u021bi, iar fi\u0219ierele duplicate sunt s\u0103rite automat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODFISCAL_TEXT = DATA_PATH.read_text(encoding=\"utf-8\")",
    "",
    "# Indexare (opera\u021bie asincron\u0103 gestionat\u0103 intern)",
    "hirag.insert(CODFISCAL_TEXT)",
    "print(\"Indexare finalizat\u0103.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Func\u021bie de chat \u0219i exemple de \u00eentreb\u0103ri\n",
    "Func\u021bia `ask()` ruleaz\u0103 c\u0103utarea ierarhic\u0103 (`mode=\"hi\"`) pentru r\u0103spunsuri argumentate. Po\u021bi schimba modul \u00een `hi_bridge`, `hi_local`, `hi_global`, `hi_nobridge` sau `naive` pentru compara\u021bie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal",
    "",
    "def ask(question: str, mode: Literal[\"hi\", \"hi_bridge\", \"hi_local\", \"hi_global\", \"hi_nobridge\", \"naive\"] = \"hi\"):",
    "    print(f\"",
    "\u00centrebare: {question}",
    "Mod: {mode}",
    "\")",
    "    answer = hirag.query(question, param=QueryParam(mode=mode))",
    "    print(f\"R\u0103spuns:",
    "{answer}\")",
    "",
    "# Exemplu de test: ajusteaz\u0103 \u00eentrebarea dup\u0103 nevoie",
    "ask(\"Care sunt regulile de deducere pentru TVA la achizi\u021biile interne?\", mode=\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sugestie:** Pentru sesiuni de chat iterative, po\u021bi salva istoricul \u00eentreb\u0103rilor \u0219i s\u0103 pre\u00eencarci `history_messages` \u00een `chat_completion` pentru a men\u021bine contextul conversa\u021biei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verificare rapid\u0103 a mediului\n",
    "Po\u021bi rula o verificare simpl\u0103 pentru a confirma c\u0103 pachetul HiRAG se import\u0103 corect \u00eenainte de a consuma credite de inferen\u021b\u0103. Comanda de mai jos compileaz\u0103 toate modulele locale \u0219i semnaleaz\u0103 eventualele erori de sintax\u0103.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test: valideaz\u0103 c\u0103 modulele HiRAG se compileaz\u0103 corect\n",
    "!python -m compileall hirag\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}